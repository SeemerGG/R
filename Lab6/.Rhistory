geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of studytime", x = "Score", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = failures)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of failures", x = "Amount", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = Dalc)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of Dalc", x = "Amount", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = Walc)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of Walc", x = "Amount", y = "Frequency") +
theme_minimal()
#  Step 2: Hierarchical clustering and dendrogram
numeric_data <- data %>% select_if(is.numeric)
data_scaled <- scale(numeric_data)
# Perform hierarchical clustering
dist_matrix <- dist(data_scaled, method = "euclidean")
# Cut the tree to get the desired number of groups
k <- 5
# Build a diagram of the "Elbow method"
fviz_nbclust(numeric_data, kmeans, method = "wss") + geom_vline(xintercept = k, linetype = 2)
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, hang = -1)
rect.hclust(hc, k = k, border = "red")
groups <- cutree(hc, k)
# Calculate the average values for each group
numeric_data$Group <- as.factor(groups)
group_means <- aggregate(. ~ Group, numeric_data, mean)
# nicely printed information for each group
kable(group_means, caption = "Mean Parameters for Each Cluster")
# Reshape the data frame to a long format
group_means_long <- melt(group_means, id.vars = "Group")
# Create a bar plot
ggplot(group_means_long, aes(x = Group, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
xlab("Groups") +
ylab("Average Value") +
ggtitle("Average Values of Characteristics by Group") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Step 3: Stone scree
# Extract the height values from the hierarchical clustering object
height_values <- as.data.frame(hc$height)
height_values$groups <- nrow(height_values):1
colnames(height_values) <- c("height", "groups")
print("Height_values")
print(height_values)
print("-------")
ggplot(height_values, aes(x = groups, y = height)) +
geom_point() +
geom_line() +
xlab("Number of Groups") +
ylab("Height") +
ggtitle("Scree Plot for Hierarchical Clustering") +
theme_minimal()
# Step 4: Scatterplot using ggplot2 (k-means clustering):
ggplot(numeric_data, aes(x = G1, y = G2, color = Group)) +
geom_point() +
labs(title = "Scatterplot of G1 vs G2", x = "G1", y = "G2") +
theme_minimal()
ggplot(numeric_data, aes(x = G1, y = G3, color = Group)) +
geom_point() +
labs(title = "Scatterplot of G1 vs G3", x = "G1", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = G2, y = G3, color = Group)) +
geom_point() +
labs(title = "Scatterplot of G2 vs G3", x = "G2", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = Dalc, y = G3, color = Group)) +
geom_point() +
labs(title = "Scatterplot of Dalc vs G3", x = "Dalc", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = Walc, y = G3, color = Group)) +
geom_point() +
labs(title = "Scatterplot of Walc vs G3", x = "Walc", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = absences, y = G3, color = Group)) +
geom_point() +
labs(title = "Scatterplot of absenses vs G3", x = "absenses", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = studytime, y = G3, color = Group)) +
geom_point() +
labs(title = "Scatterplot of studytime vs G3", x = "studytime", y = "G3") +
theme_minimal()
# Create a 3D scatterplot
colors <- as.integer(numeric_data$Group)
scatterplot3d(x = numeric_data$G1, y = numeric_data$G2, z = numeric_data$G3,
color = colors, pch = 19,
xlab = "G1", ylab = "G2", zlab = "G3",
main = "3D Scatterplot of G1, G2, and G3")
# print amount of students in each group
print(table(groups))
# Lab 6.2
# Add the classes (groups) from the clustering analysis as a new column
data$Group <- as.factor(groups)
# Split the data into training and test sets
set.seed(123)
train_index <- createDataPartition(data$Group, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train a naive Bayes classifier
nb_model <- naiveBayes(Group ~ ., data = train_data)
# Predict the test data using the naive Bayes classifier
test_data$predicted_group <- predict(nb_model, test_data)
# Calculate the accuracy of the predictions
confusion_matrix <- confusionMatrix(test_data$predicted_group, test_data$Group)
accuracy <- confusion_matrix$overall['Accuracy']
print(accuracy)
# Analyze the accuracy of the obtained solutions for the test data
print(confusion_matrix)
# 1. Apply the decision tree method to the classification problem
dt_model <- rpart(Group ~ ., data = train_data, method = "class")
# 2. Explore the decision tree and plot it, if dimensionality permits
print(dt_model)
print("-------------")
# 3. Analyze the accuracy of the obtained solutions for the test data
test_data$predicted_group_dt <- predict(dt_model, test_data, type = "class")
confusion_matrix_dt <- confusionMatrix(test_data$predicted_group_dt, test_data$Group)
accuracy_dt <- confusion_matrix_dt$overall['Accuracy']
print("-------------")
print(accuracy_dt)
print(confusion_matrix_dt)
print("-------------")
# 4. Perform classification using a random forest
rf_model <- randomForest(Group ~ ., data = train_data, importance = TRUE)
test_data$predicted_group_rf <- predict(rf_model, test_data)
confusion_matrix_rf <- confusionMatrix(test_data$predicted_group_rf, test_data$Group)
accuracy_rf <- confusion_matrix_rf$overall['Accuracy']
print(accuracy_rf)
print(confusion_matrix_rf)
print("-------------")
# 5. Compare the results with the results of the Bayesian classifier
cat("Naive Bayes Accuracy:", accuracy, "\n")
cat("Decision Tree Accuracy:", accuracy_dt, "\n")
cat("Random Forest Accuracy:", accuracy_rf, "\n")
# Plot the decision tree
rpart.plot(dt_model, type = 4, extra = 101, tweak = 1.2, main = "Decision Tree")
# Plot the variable importance for the random forest
rf_var_imp <- importance(rf_model)
rf_var_imp_df <- data.frame(Feature = rownames(rf_var_imp), Importance = rf_var_imp[, 1])
ggplot(rf_var_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Random Forest Variable Importance", x = "Feature", y = "Importance") +
theme_minimal() +
coord_flip()
library(purrr) #подключаем пакет для работы функции map
path = paste(getwd(),"/GitHub/R/Lab2/answers.csv", sep = "") #получаем путь к файлу с данными
data <- data.frame(read.csv(path)) #создаем объект data frame
colMeans(data[seq(3, ncol(data))], na.rm = TRUE, dims = 1) #вычисляем mean по каждому столбцу
map(data[seq(3, ncol(data))],function(x) max(x, na.rm = TRUE)) #получаем вектор максимальных значений для каждого столбца
map(data[seq(3, ncol(data))],function(x) min(x, na.rm = TRUE)) #получаем вектор минимальных значений для каждого столбца
map(data[seq(3, ncol(data))], function(x) length(x[x>7])) #количество типо которым фильм понравился
map(data[seq(3, ncol(data))], function(x) length(x[x<3])) # количество людей которым мало наличия Гослинга, чтобы поставить оценку больше 7
sort(colMeans(data[seq(3, ncol(data))], na.rm = TRUE, dims = 1), decreasing = TRUE) #выводим рейтинг фильмов по убыванию
barplot(height = colMeans(data[seq(3, ncol(data))], na.rm = TRUE, dims = 1)) #делаем столбчатую диаграмму
str(data)
# Step 1: Descriptive analysis of the data
summary(data)
df <- read.csv('dataset.csv')
View(df)
View(df)
df[, c("LONCOD", "LATCOD", "CDCODE", "city", "address", "date", "NCESSCH", "time") := NULL]
df <- df[,c("year", "city", "state", "urbanrural", "race", "killed", "school", "injured", "victims", "type")]
summary(df)
df <- df[,c("year", "city", "state", "urbanrural", "race", "killed", "injured", "victims", "type")]
summary(df)
df <- read.csv('student-mat.csv')
summary(df)
df <- read.csv('dataset.csv')
df <- df[,c("year", "city", "state", "urbanrural", "race", "killed", "injured", "victims", "type")]
summary(df)
#Количество событий по регионам
boxplot(df)
df <- read.csv('student-mat.csv')
summary(df)
ggplot(data, aes(x = G1)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of G1", x = "Score", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = G2)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of G2", x = "Score", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = G3)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of G3", x = "Score", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = absences)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of absences", x = "Amount", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = studytime)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of studytime", x = "Score", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = absences)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Histogram of absences", x = "Amount", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = absences)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Гистограмма прогулов", x = "Количество", y = "Частота") +
theme_minimal()
ggplot(data, aes(x = studytime)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Количество времени уделяемого учебе", x = "Score", y = "Frequency") +
theme_minimal()
ggplot(data, aes(x = studytime)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Количество времени уделяемого учебе", x = "Часы 1*10ч", y = "Частота") +
theme_minimal()
ggplot(data, aes(x = absences)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Гистограмма прогулов", x = "Количество", y = "Частота") +
theme_minimal()
ggplot(df, aes(x = absences)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Гистограмма школьных прогулов", x = "Количество", y = "Частота") +
theme_minimal()
ggplot(huy, aes(x = absences)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Гистограмма школьных прогулов", x = "Количество", y = "Частота") +
theme_minimal()
data <- read.csv('student-mat.csv')
summary(data)
data[,c('G1', "G2", "G3")]
boxplot(c('G1', "G2", "G3"), data[,c('G1', "G2", "G3")])
boxplot(c('G1', "G2", "G3"), data[,c('G1')])
boxplot(data[,c('G1')])
boxplot(data[,c('G1', 'G2', 'G3')])
boxplot(data[,c('G1', 'G2', 'G3')], main='Распределение оценок')
summary(data)
ggplot(data, aes(x = absences)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Гистограмма школьных прогулов", x = "Количество", y = "Частота") +
theme_minimal()
ggplot(data, aes(x = studytime)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Гистограмма времени уделяемого учебе", x = "Часы 1*10ч", y = "Частота") +
theme_minimal()
ggplot(data, aes(x = failures)) +
geom_histogram(color = "black", fill = "lightblue", bins = 20) +
labs(title = "Гистограмма отказов", x = "Количество", y = "Частота") +
theme_minimal()
numeric_data <- data %>% select_if(is.numeric)
View(confusion_matrix)
View(confusion_matrix_dt)
View(confusion_matrix_rf)
View(data)
View(data_scaled)
View(numeric_data)
data_scaled <- scale(numeric_data)
#Выполняем иерархическую кластеризацию
dist_matrix <- dist(data_scaled, method = "euclidean")
k<- 5
fviz_nbclust(numeric_data, kmeans, method = "wss") + geom_vline(xintercept = k, linetype = 2)
fviz_nbclust(numeric_data, kmeans, method = "wss") + geom_vline(xintercept = 10, linetype = 2)
#Выполняем иерархическую кластеризацию
dist_matrix <- dist(data_scaled, method = "euclidean")
k<- 5
fviz_nbclust(numeric_data, kmeans, method = "wss") + geom_vline(xintercept = k, linetype = 2)
#Выбираем количество групп. Строим локтевую диаграмму.
fviz_nbclust(numeric_data, kmeans, method = "wss") + geom_vline( linetype = 2)
#Выбираем количество групп. Строим локтевую диаграмму.
fviz_nbclust(numeric_data, kmeans, method = "wss")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, hang = -1)
rect.hclust(hc, k = k, border = "red")
groups <- cutree(hc, k)
#Расчитываем среднее значение для каждой группы
numeric_data$Group <- as.factor(groups)
group_means <- aggregate(. ~ Group, numeric_data, mean)
View(group_means)
View(group_means)
kable(group_means, caption = "Mean Parameters for Each Cluster")
group_means_long <- melt(group_means, id.vars = "Group")
ggplot(group_means_long, aes(x = Group, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
xlab("Groups") +
ylab("Average Value") +
ggtitle("Average Values of Characteristics by Group") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(group_means_long, aes(x = Group, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
xlab("Номер группы") +
ylab("Среднее значение") +
ggtitle("Среднии значения характеристик для каждой группы") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(group_means_long, aes(x = Group, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
xlab("Номер группы") +
ylab("Среднее значение") +
ggtitle("Среднии значения характеристик для каждой группы") +
variable.names('jdfk') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(group_means_long, aes(x = Group, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
xlab("Номер группы") +
ylab("Среднее значение") +
ggtitle("Среднии значения характеристик для каждой группы") +
variable('jdfk') +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
fviz_nbclust(numeric_data, kmeans, method = "wss")
#Строим дендограмму выделяем к групп
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, hang = -1)
plot(hc, hang = -1, main="двалпо")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, hang = -1, main="Дендограмма характеристик студентов")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, hang = -1, main="Дендрограмма характеристик студентов")
View(data_scaled)
fviz_nbclust(numeric_data, kmeans, method = "wss")
rect.hclust(hc, k = k, border = "red")
groups <- cutree(hc, k)
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, hang = -1, main="Дендрограмма характеристик студентов")
rect.hclust(hc, k = k, border = "red")
groups <- cutree(hc, k)
group_means_long <- melt(group_means, id.vars = "Group")
ggplot(group_means_long, aes(x = Group, y = value, fill = variable)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
xlab("Номер группы") +
ylab("Среднее значение") +
ggtitle("Среднии значения характеристик для каждой группы") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
kable(group_means, caption = "Mean Parameters for Each Cluster")
height_values <- as.data.frame(hc$height)
height_values$groups <- nrow(height_values):1
colnames(height_values) <- c("height", "groups")
print("Height_values")
print(height_values)
print("-------")
ggplot(height_values, aes(x = groups, y = height)) +
geom_point() +
geom_line() +
xlab("Number of Groups") +
ylab("Height") +
ggtitle("Scree Plot for Hierarchical Clustering") +
theme_minimal()
height_values <- as.data.frame(hc$height)
height_values$groups <- nrow(height_values):1
colnames(height_values) <- c("height", "groups")
ggplot(height_values, aes(x = groups, y = height)) +
geom_point() +
geom_line() +
xlab("Номер группы") +
ylab("Высота") +
ggtitle("Каменная осыпь") +
theme_minimal()
erplot (k-means кластеризация)
ggplot(numeric_data, aes(x = G1, y = G2, color = Group)) +
geom_point() +
labs(title = "Scatterplot of G1 vs G2", x = "G1", y = "G2") +
theme_minimal()
ggplot(numeric_data, aes(x = G1, y = G2, color = Group)) +
geom_point() +
labs(title = "Scatterplot of G1 vs G2", x = "G1", y = "G2") +
theme_minimal()
ggplot(numeric_data, aes(x = G1, y = G2, color = Group)) +
geom_point() +
labs(title = "Диаграмма рассеивания для G1 и G2", x = "G1", y = "G2") +
theme_minimal()
ggplot(numeric_data, aes(x = G1, y = G3, color = Group)) +
geom_point() +
labs(title = "Диаграмма рассеивания для G1 и G3", x = "G1", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = G2, y = G3, color = Group)) +
geom_point() +
labs(title = "Диаграмма рассеивания для G2 и G3", x = "G2", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = Dalc, y = G3, color = Group)) +
geom_point() +
labs(title = "Диаграмма рассеивания для Dalc и G3", x = "Dalc", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = Walc, y = G3, color = Group)) +
geom_point() +
labs(title = "Диаграмма рассеивания для Walc и G3", x = "Walc", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = absences, y = G3, color = Group)) +
geom_point() +
labs(title = "Диаграмма рассеивания для absenses и G3", x = "absenses", y = "G3") +
theme_minimal()
ggplot(numeric_data, aes(x = studytime, y = G3, color = Group)) +
geom_point() +
labs(title = "Диаграмма рассеивания для studytime и G3", x = "studytime", y = "G3") +
theme_minimal()
colors <- as.integer(numeric_data$Group)
scatterplot3d(x = numeric_data$G1, y = numeric_data$G2, z = numeric_data$G3,
color = colors, pch = 19,
xlab = "G1", ylab = "G2", zlab = "G3",
main = "3D Scatterplot of G1, G2, and G3")
colors <- as.integer(numeric_data$Group)
scatterplot3d(x = numeric_data$G1, y = numeric_data$G2, z = numeric_data$G3,
color = colors, pch = 19,
xlab = "G1", ylab = "G2", zlab = "G3",
main = "3D Диаграмма рассеивания G1, G2, и G3")
View(numeric_data)
View(numeric_data)
#Часть 2
data$Group <- as.factor(groups)
View(data)
View(data)
#Делим данные на обучающие и тестовые
set.seed(123)
train_index <- createDataPartition(data$Group, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
#оБУчаем наивного баеса
nb_model <- naiveBayes(Group ~ ., data = train_data)
#Прогнозируем
test_data$predicted_group <- predict(nb_model, test_data)
confusion_matrix <- confusionMatrix(test_data$predicted_group, test_data$Group)
accuracy <- confusion_matrix$overall['Accuracy']
print(accuracy)
print(confusion_matrix)
confusion_matrix <- confusionMatrix(test_data$predicted_group, test_data$Group)
accuracy <- confusion_matrix$overall['Accuracy']
print(accuracy)
print(confusion_matrix)
#Реализация деревьев решений
dt_model <- rpart(Group ~ ., data = train_data, method = "class")
print(dt_model)
test_data$predicted_group_dt <- predict(dt_model, test_data, type = "class")
confusion_matrix_dt <- confusionMatrix(test_data$predicted_group_dt, test_data$Group)
accuracy_dt <- confusion_matrix_dt$overall['Accuracy']
print(accuracy_dt)
print(confusion_matrix_dt)
rf_model <- randomForest(Group ~ ., data = train_data, importance = TRUE)
test_data$predicted_group_rf <- predict(rf_model, test_data)
confusion_matrix_rf <- confusionMatrix(test_data$predicted_group_rf, test_data$Group)
accuracy_rf <- confusion_matrix_rf$overall['Accuracy']
print(accuracy_rf)
print(confusion_matrix_rf)
rf_model <- randomForest(Group ~ ., data = train_data, importance = TRUE)
test_data$predicted_group_rf <- predict(rf_model, test_data)
confusion_matrix_rf <- confusionMatrix(test_data$predicted_group_rf, test_data$Group)
accuracy_rf <- confusion_matrix_rf$overall['Accuracy']
print(accuracy_rf)
print(confusion_matrix_rf)
rf_model <- randomForest(Group ~ ., data = train_data, importance = TRUE)
test_data$predicted_group_rf <- predict(rf_model, test_data)
confusion_matrix_rf <- confusionMatrix(test_data$predicted_group_rf, test_data$Group)
accuracy_rf <- confusion_matrix_rf$overall['Accuracy']
print(accuracy_rf)
print(confusion_matrix_rf)
#
rf_var_imp <- importance(rf_model)
rf_var_imp_df <- data.frame(Feature = rownames(rf_var_imp), Importance = rf_var_imp[, 1])
ggplot(rf_var_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Random Forest Variable Importance", x = "Feature", y = "Importance") +
theme_minimal() +
coord_flip()
#
rpart.plot(dt_model, type = 4, extra = 101, tweak = 1.2, main = "Decision Tree")
dt_model <- rpart(Group ~ ., data = train_data, method = "class")
print(dt_model)
test_data$predicted_group_dt <- predict(dt_model, test_data, type = "class")
confusion_matrix_dt <- confusionMatrix(test_data$predicted_group_dt, test_data$Group)
accuracy_dt <- confusion_matrix_dt$overall['Accuracy']
print(accuracy_dt)
print(confusion_matrix_dt)
#
rpart.plot(dt_model, type = 4, extra = 101, tweak = 1.2, main = "Decision Tree")
#
rpart.plot(dt_model, type = 4, extra = 101, tweak = 1.2, main = "Древо решений")
cat("Naive Bayes Accuracy:", accuracy, "\n")
cat("Decision Tree Accuracy:", accuracy_dt, "\n")
cat("Random Forest Accuracy:", accuracy_rf, "\n")
rf_model <- randomForest(Group ~ ., data = train_data, importance = TRUE)
test_data$predicted_group_rf <- predict(rf_model, test_data)
confusion_matrix_rf <- confusionMatrix(test_data$predicted_group_rf, test_data$Group)
accuracy_rf <- confusion_matrix_rf$overall['Accuracy']
print(accuracy_rf)
print(confusion_matrix_rf)
rf_var_imp <- importance(rf_model)
rf_var_imp_df <- data.frame(Feature = rownames(rf_var_imp), Importance = rf_var_imp[, 1])
ggplot(rf_var_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "lightgeen") +
labs(title = "График важности переменной в случайном лесу", x = "Feature", y = "Importance") +
theme_minimal() +
coord_flip()
rf_var_imp <- importance(rf_model)
rf_var_imp_df <- data.frame(Feature = rownames(rf_var_imp), Importance = rf_var_imp[, 1])
ggplot(rf_var_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "black") +
labs(title = "График важности переменной в случайном лесу", x = "Feature", y = "Importance") +
theme_minimal() +
coord_flip()
rf_var_imp <- importance(rf_model)
rf_var_imp_df <- data.frame(Feature = rownames(rf_var_imp), Importance = rf_var_imp[, 1])
ggplot(rf_var_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "lightgreen") +
labs(title = "График важности переменной в случайном лесу", x = "Feature", y = "Importance") +
theme_minimal() +
coord_flip()
rf_var_imp <- importance(rf_model)
rf_var_imp_df <- data.frame(Feature = rownames(rf_var_imp), Importance = rf_var_imp[, 1])
ggplot(rf_var_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "lightgreen") +
labs(title = "График важности переменной в случайном лесу", x = "Особенность", y = "Коэффициент важности") +
theme_minimal() +
coord_flip()
